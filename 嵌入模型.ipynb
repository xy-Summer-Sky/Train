{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:50.579193Z",
     "start_time": "2024-11-08T04:59:39.348644Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/root/train/Train']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/train39/lib/python3.9/site-packages/IPython/core/async_helpers.py:129\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/train39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3248\u001b[0m _run_async \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 3251\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3253\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_trap:\n\u001b[1;32m   3254\u001b[0m         \u001b[38;5;66;03m# Compile to bytecode\u001b[39;00m\n\u001b[1;32m   3255\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/train39/lib/python3.9/site-packages/IPython/core/compilerop.py:155\u001b[0m, in \u001b[0;36mCachingCompiler.cache\u001b[0;34m(self, transformed_code, number, raw_code)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     raw_code \u001b[38;5;241m=\u001b[39m transformed_code\n\u001b[0;32m--> 155\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_code_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Save the execution count\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_map[name] \u001b[38;5;241m=\u001b[39m number\n",
      "File \u001b[0;32m~/miniconda3/envs/train39/lib/python3.9/site-packages/ipykernel/compiler.py:105\u001b[0m, in \u001b[0;36mXCachingCompiler.get_code_name\u001b[0;34m(self, raw_code, code, number)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_code_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_code, code, number):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the code name.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_file_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/train39/lib/python3.9/site-packages/ipykernel/compiler.py:91\u001b[0m, in \u001b[0;36mget_file_name\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cell_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     name \u001b[38;5;241m=\u001b[39m murmur2_x86(code, get_tmp_hash_seed())\n\u001b[0;32m---> 91\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_tmp_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell_name\n",
      "File \u001b[0;32m~/miniconda3/envs/train39/lib/python3.9/site-packages/ipykernel/compiler.py:76\u001b[0m, in \u001b[0;36mget_tmp_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tmp_directory\u001b[39m():\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a temp directory.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     tmp_dir \u001b[38;5;241m=\u001b[39m convert_to_long_pathname(\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgettempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m     pid \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid()\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tmp_dir \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipykernel_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(pid)\n",
      "File \u001b[0;32m~/miniconda3/envs/train39/lib/python3.9/tempfile.py:307\u001b[0m, in \u001b[0;36mgettempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tempdir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m         tempdir \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_tempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     _once_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/train39/lib/python3.9/tempfile.py:223\u001b[0m, in \u001b[0;36m_get_default_tempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m   \u001b[38;5;66;03m# no point trying more names in this directory\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(_errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    224\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo usable temporary directory found in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    225\u001b[0m                         dirlist)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/root/train/Train']"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter,TokenTextSplitter\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers.bm25 import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "DOCS_DIR = './A/A_document'\n",
    "EMB_MODEL = './bge-large-zh-v1.5'\n",
    "RERANK_MODEL = \"./bge-reranker-large\"\n",
    "PERSIST_DIR = './vectordb' \n",
    "QUERY_DIR = './A/A_question.csv'\n",
    "SUB_DIR = './submit_example.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:50.657198Z",
     "start_time": "2024-11-08T04:59:50.596199Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query = pd.read_csv(QUERY_DIR)\n",
    "sub = pd.read_csv(\"./submit_example.csv\")\n",
    "display(query.head(3))\n",
    "display(sub.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF文档解析和切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:33.751532Z",
     "start_time": "2024-11-08T04:59:51.353879Z"
    }
   },
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(DOCS_DIR)\n",
    "pages = loader.load_and_split()\n",
    "pdf_list = os.listdir(DOCS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:33.798171Z",
     "start_time": "2024-11-08T05:00:33.764543Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_text = { pdf_page.metadata['source'][-8:]:'' for pdf_page  in pages }\n",
    "for pdf in tqdm(pdf_list):\n",
    "    for pdf_page in pages:\n",
    "        if pdf in pdf_page.metadata['source']:\n",
    "            pdf_text[pdf] += pdf_page.page_content\n",
    "        else:\n",
    "            continue\n",
    "print('key:pdf value:text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:33.827680Z",
     "start_time": "2024-11-08T05:00:33.814687Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    # 页码清除 效果不好\n",
    "#     page_id_pattern1 = r'\\n\\d+\\s*/\\s*\\d+\\s*\\n'\n",
    "#     page_id_pattern2 = r'\\n\\d+\\n'\n",
    "#     page_id_pattern3 = r'\\n\\d+\\s*?'\n",
    "\n",
    "#     page_id_pattern = page_id_pattern1+'|'+page_id_pattern2+'|'+page_id_pattern3\n",
    "#     text = re.sub(page_id_pattern,'',text)\n",
    "    \n",
    "    # '\\n', ' ' 删除\n",
    "    text = text.replace('\\n','').replace(' ','')\n",
    "    \n",
    "    # 删除页码\n",
    "    \n",
    "    # 删除本文档为2024CCFBDC***\n",
    "    head_pattern = '本文档为2024CCFBDCI比赛用语料的一部分。[^\\s]+仅允许在本次比赛中使用。'\n",
    "    # news_pattern\n",
    "    pattern1 = r\"发布时间：[^\\s]+发布人：新闻宣传中心\"\n",
    "    pattern2 = r\"发布时间：[^\\s]+发布人：新闻发布人\"\n",
    "    pattern3 =  r'发布时间：\\d{4}年\\d{1,2}月\\d{1,2}日'\n",
    "    news_pattern = head_pattern+'|'+pattern1+'|'+pattern2+'|'+pattern3\n",
    "    text = re.sub(news_pattern,'',text)\n",
    "    \n",
    "    \n",
    "    # report_pattern\n",
    "    report_pattern1 = '第一节重要提示[^\\s]+本次利润分配方案尚需提交本公司股东大会审议。'\n",
    "    report_pattern12 = '一重要提示[^\\s]+股东大会审议。'\n",
    "    report_pattern13 = '一、重要提示[^\\s]+季度报告未经审计。'\n",
    "    report_pattern2 = '本公司董事会及全体董事保证本公告内容不存在任何虚假记载、[^\\s]+季度财务报表是否经审计□是√否'\n",
    "    report_pattern3 = '中国联合网络通信股份有限公司（简称“公司”）董事会审计委员会根据相关法律法规、[^\\s]+汇报如下：'\n",
    "    report_pattern = report_pattern1+'|'+report_pattern12+'|'+report_pattern13+'|'+report_pattern2+'|'+report_pattern3\n",
    "    text = re.sub( report_pattern,'',text)\n",
    "#     white paper 版本一 效果不好\n",
    "    # 优先级别 bp1 bp2 bp3\n",
    "#     bp_pattern_law = '版权声明[^\\s]+追究其相关法律责任。'\n",
    "#     bp_pattern1 = r'目录.*?披露发展报告（\\d{4}年）' # 只针对AZ08.pdf\n",
    "#     bp_pattern2 = r'目录.*?白皮书.*?（\\d{4}年）'\n",
    "#     bp_pattern3 = r'目录.*?白皮书'\n",
    "#     bp_pattern = bp_pattern_law  +'|'+bp_pattern1+'|'+bp_pattern2+'|'+bp_pattern3\n",
    "#     text = re.sub(bp_pattern,'',text)\n",
    "    \n",
    "#     print(text)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:33.891679Z",
     "start_time": "2024-11-08T05:00:33.845683Z"
    }
   },
   "outputs": [],
   "source": [
    "for pdf_id in pdf_text.keys():\n",
    "    pdf_text[pdf_id] = filter_text(pdf_text[pdf_id])\n",
    "with open('AZ.txt','w',encoding = 'utf-8') as file:\n",
    "    pdf_all = ''.join(list(pdf_text.values())).encode('utf-8', 'replace').decode('utf-8')\n",
    "    file.write( pdf_all)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:33.986475Z",
     "start_time": "2024-11-08T05:00:33.910830Z"
    }
   },
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import TextLoader\n",
    "# import re\n",
    "# from typing import List\n",
    "\n",
    "# class ChineseTextSplitter(CharacterTextSplitter):\n",
    "#     def __init__(self, pdf: bool = False, sentence_size: int = 250, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.pdf = pdf\n",
    "#         self.sentence_size = sentence_size\n",
    "\n",
    "#     def split_text(self, text: str) -> List[str]:\n",
    "#         if self.pdf:\n",
    "#             text = re.sub(r\"\\n{3,}\", \"\\n\", text)\n",
    "#             text = re.sub('\\s', ' ', text)\n",
    "#             text = text.replace(\"\\n\\n\", \"\")\n",
    "#         sent_sep_pattern = re.compile('([﹒﹔﹖﹗．。！？][\"’”」』]{0,2}|(?=[\"‘“「『]{1,2}|$))')  # del ：；\n",
    "#         sent_list = []\n",
    "#         for ele in sent_sep_pattern.split(text):\n",
    "#             if sent_sep_pattern.match(ele) and sent_list:\n",
    "#                 sent_list[-1] += ele\n",
    "#             elif ele:\n",
    "#                 sent_list.append(ele)\n",
    "#         return sent_list\n",
    "\n",
    "# # Load text from file\n",
    "# loader = TextLoader(\"AZ.txt\", encoding=\"utf-8\")\n",
    "# documents = loader.load()\n",
    "\n",
    "# # Split text using ChineseTextSplitter\n",
    "# text_splitter = ChineseTextSplitter(\n",
    "#     pdf=False,\n",
    "#     sentence_size=250,\n",
    "#     chunk_size=300,\n",
    "#     chunk_overlap=100,\n",
    "#     keep_separator='end'\n",
    "# )\n",
    "import json\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.schema import Document  # Import the correct Document class\n",
    "\n",
    "# Load the segmented documents from the JSON file\n",
    "with open('segmented_docs.json', 'r', encoding='utf-8') as f:\n",
    "    segmented_docs = json.load(f)\n",
    "\n",
    "# Convert the loaded JSON data to the required format\n",
    "docs = [Document(page_content=doc['page_content'], metadata=doc['metadata']) for doc in segmented_docs]\n",
    "\n",
    "# Now `docs` contains the segmented documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:34.032943Z",
     "start_time": "2024-11-08T05:00:34.018943Z"
    }
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本块向量化（比赛限定使用bge-large-zh-v1.5模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T05:00:34.066942Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL, show_progress=True)\n",
    "vectordb = FAISS.from_documents(   \n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "vectordb.save_local(PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混合检索器\n",
    "\n",
    "#### bm25 \n",
    "- k1 较高的 k1 值意味着词频对评分的影响更大。\n",
    "- b  当 b=1 时，文档长度的影响最大；当b = 0 时，文档长度不影响评分。\n",
    "- langchain 默认切分英文split()，中文需要jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:29.647477200Z",
     "start_time": "2024-11-08T03:41:11.920515Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "dense_retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    docs, \n",
    "    k=5, \n",
    "    bm25_params={\"k1\": 1.5, \"b\": 0.75}, \n",
    "    preprocess_func=jieba.lcut\n",
    ")\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, dense_retriever], weights=[0.4, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本召回和重排"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:29.648479200Z",
     "start_time": "2024-11-08T03:57:24.733084Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "def rerank(questions, retriever, top_n=1, cut_len=384):\n",
    "    rerank_model = HuggingFaceCrossEncoder(model_name=RERANK_MODEL)\n",
    "    compressor = CrossEncoderReranker(model=rerank_model, top_n=top_n)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )\n",
    "    rerank_answers = []\n",
    "    for question in tqdm(questions):\n",
    "        relevant_docs = compression_retriever.invoke(question)\n",
    "        answer=''\n",
    "        for rd in relevant_docs:\n",
    "            answer += rd.page_content\n",
    "        rerank_answers.append(answer[:245])\n",
    "    return rerank_answers\n",
    "\n",
    "questions = list(query['question'].values)\n",
    "rerank_answers = rerank(questions, ensemble_retriever)\n",
    "print(rerank_answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 提交"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:29.648479200Z",
     "start_time": "2024-11-07T17:51:49.193140Z"
    }
   },
   "outputs": [],
   "source": [
    "def emb(answers, emb_batch_size=4):\n",
    "    model = SentenceTransformer(EMB_MODEL, trust_remote_code=True)\n",
    "    all_sentence_embeddings = []\n",
    "    for i in tqdm(range(0, len(answers), emb_batch_size), desc=\"embedding sentences\"):\n",
    "        batch_sentences = answers[i:i+emb_batch_size]\n",
    "        sentence_embeddings = model.encode(batch_sentences, normalize_embeddings=True)\n",
    "        all_sentence_embeddings.append(sentence_embeddings)\n",
    "    all_sentence_embeddings = np.concatenate(all_sentence_embeddings, axis=0)\n",
    "    print('emb_model max_seq_length: ', model.max_seq_length)\n",
    "    print('emb_model embeddings_shape: ', all_sentence_embeddings.shape[-1])\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return all_sentence_embeddings\n",
    "\n",
    "all_sentence_embeddings = emb(rerank_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:29.648479200Z",
     "start_time": "2024-11-07T17:51:52.309769Z"
    }
   },
   "outputs": [],
   "source": [
    "sub['answer'] = rerank_answers\n",
    "sub['embedding']= [','.join([str(a) for a in all_sentence_embeddings[i]]) for i in range(len(all_sentence_embeddings))]\n",
    "sub.to_csv('submit5.0.csv', index=None)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5782812,
     "sourceId": 9501900,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5819690,
     "sourceId": 9551539,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5831812,
     "sourceId": 9568175,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (train39)",
   "language": "python",
   "name": "train39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
