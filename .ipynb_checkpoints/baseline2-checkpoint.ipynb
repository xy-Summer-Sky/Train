{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:50.579193Z",
     "start_time": "2024-11-08T04:59:39.348644Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.text_splitters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFDirectoryLoader, TextLoader\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter, CharacterTextSplitter, TokenTextSplitter\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Retriever, EnsembleRetriever\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.text_splitters'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter,TokenTextSplitter\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers.bm25 import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "DOCS_DIR = './A/A_document'\n",
    "EMB_MODEL = './bge-large-zh-v1.5'\n",
    "RERANK_MODEL = \"./bge-reranker-large\"\n",
    "PERSIST_DIR = './vectordb' \n",
    "QUERY_DIR = './A/A_question.csv'\n",
    "SUB_DIR = './submit_example.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:50.657198Z",
     "start_time": "2024-11-08T04:59:50.596199Z"
    }
   },
   "outputs": [],
   "source": [
    "query = pd.read_csv(QUERY_DIR)\n",
    "sub = pd.read_csv(\"./submit_example.csv\")\n",
    "display(query.head(3))\n",
    "display(sub.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF文档解析和切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:33.751532Z",
     "start_time": "2024-11-08T04:59:51.353879Z"
    }
   },
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(DOCS_DIR)\n",
    "pages = loader.load_and_split()\n",
    "pdf_list = os.listdir(DOCS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:33.798171Z",
     "start_time": "2024-11-08T05:00:33.764543Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_text = { pdf_page.metadata['source'][-8:]:'' for pdf_page  in pages }\n",
    "for pdf in tqdm(pdf_list):\n",
    "    for pdf_page in pages:\n",
    "        if pdf in pdf_page.metadata['source']:\n",
    "            pdf_text[pdf] += pdf_page.page_content\n",
    "        else:\n",
    "            continue\n",
    "print('key:pdf value:text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:33.827680Z",
     "start_time": "2024-11-08T05:00:33.814687Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    # 页码清除 效果不好\n",
    "#     page_id_pattern1 = r'\\n\\d+\\s*/\\s*\\d+\\s*\\n'\n",
    "#     page_id_pattern2 = r'\\n\\d+\\n'\n",
    "#     page_id_pattern3 = r'\\n\\d+\\s*?'\n",
    "\n",
    "#     page_id_pattern = page_id_pattern1+'|'+page_id_pattern2+'|'+page_id_pattern3\n",
    "#     text = re.sub(page_id_pattern,'',text)\n",
    "    \n",
    "    # '\\n', ' ' 删除\n",
    "    text = text.replace('\\n','').replace(' ','')\n",
    "    \n",
    "    # 删除页码\n",
    "    \n",
    "    # 删除本文档为2024CCFBDC***\n",
    "    head_pattern = '本文档为2024CCFBDCI比赛用语料的一部分。[^\\s]+仅允许在本次比赛中使用。'\n",
    "    # news_pattern\n",
    "    pattern1 = r\"发布时间：[^\\s]+发布人：新闻宣传中心\"\n",
    "    pattern2 = r\"发布时间：[^\\s]+发布人：新闻发布人\"\n",
    "    pattern3 =  r'发布时间：\\d{4}年\\d{1,2}月\\d{1,2}日'\n",
    "    news_pattern = head_pattern+'|'+pattern1+'|'+pattern2+'|'+pattern3\n",
    "    text = re.sub(news_pattern,'',text)\n",
    "    \n",
    "    \n",
    "    # report_pattern\n",
    "    report_pattern1 = '第一节重要提示[^\\s]+本次利润分配方案尚需提交本公司股东大会审议。'\n",
    "    report_pattern12 = '一重要提示[^\\s]+股东大会审议。'\n",
    "    report_pattern13 = '一、重要提示[^\\s]+季度报告未经审计。'\n",
    "    report_pattern2 = '本公司董事会及全体董事保证本公告内容不存在任何虚假记载、[^\\s]+季度财务报表是否经审计□是√否'\n",
    "    report_pattern3 = '中国联合网络通信股份有限公司（简称“公司”）董事会审计委员会根据相关法律法规、[^\\s]+汇报如下：'\n",
    "    report_pattern = report_pattern1+'|'+report_pattern12+'|'+report_pattern13+'|'+report_pattern2+'|'+report_pattern3\n",
    "    text = re.sub( report_pattern,'',text)\n",
    "#     white paper 版本一 效果不好\n",
    "    # 优先级别 bp1 bp2 bp3\n",
    "#     bp_pattern_law = '版权声明[^\\s]+追究其相关法律责任。'\n",
    "#     bp_pattern1 = r'目录.*?披露发展报告（\\d{4}年）' # 只针对AZ08.pdf\n",
    "#     bp_pattern2 = r'目录.*?白皮书.*?（\\d{4}年）'\n",
    "#     bp_pattern3 = r'目录.*?白皮书'\n",
    "#     bp_pattern = bp_pattern_law  +'|'+bp_pattern1+'|'+bp_pattern2+'|'+bp_pattern3\n",
    "#     text = re.sub(bp_pattern,'',text)\n",
    "    \n",
    "#     print(text)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:33.891679Z",
     "start_time": "2024-11-08T05:00:33.845683Z"
    }
   },
   "outputs": [],
   "source": [
    "for pdf_id in pdf_text.keys():\n",
    "    pdf_text[pdf_id] = filter_text(pdf_text[pdf_id])\n",
    "with open('AZ.txt','w',encoding = 'utf-8') as file:\n",
    "    pdf_all = ''.join(list(pdf_text.values())).encode('utf-8', 'replace').decode('utf-8')\n",
    "    file.write( pdf_all)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:33.986475Z",
     "start_time": "2024-11-08T05:00:33.910830Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load spacy Chinese model\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "# Function to split text into sentences using spacy\n",
    "def spacy_splitter(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "# Load documents\n",
    "loader = TextLoader(\"AZ.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents into sentences\n",
    "docs = []\n",
    "for doc in documents:\n",
    "    sentences = spacy_splitter(doc.page_content)\n",
    "    for sentence in sentences:\n",
    "        docs.append({\"page_content\": sentence, \"metadata\": doc.metadata})\n",
    "\n",
    "# Now `docs` contains the split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T05:00:34.032943Z",
     "start_time": "2024-11-08T05:00:34.018943Z"
    }
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本块向量化（比赛限定使用bge-large-zh-v1.5模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T05:00:34.066942Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL, show_progress=True)\n",
    "vectordb = FAISS.from_documents(   \n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "vectordb.save_local(PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混合检索器\n",
    "\n",
    "#### bm25 \n",
    "- k1 较高的 k1 值意味着词频对评分的影响更大。\n",
    "- b  当 b=1 时，文档长度的影响最大；当b = 0 时，文档长度不影响评分。\n",
    "- langchain 默认切分英文split()，中文需要jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:29.647477200Z",
     "start_time": "2024-11-08T03:41:11.920515Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "dense_retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    docs, \n",
    "    k=5, \n",
    "    bm25_params={\"k1\": 1.6, \"b\": 0.75}, \n",
    "    preprocess_func=jieba.lcut\n",
    ")\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, dense_retriever], weights=[0.4, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本召回和重排"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:29.648479200Z",
     "start_time": "2024-11-08T03:57:24.733084Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "def rerank(questions, retriever, top_n=1, cut_len=384):\n",
    "    rerank_model = HuggingFaceCrossEncoder(model_name=RERANK_MODEL)\n",
    "    compressor = CrossEncoderReranker(model=rerank_model, top_n=top_n)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )\n",
    "    rerank_answers = []\n",
    "    for question in tqdm(questions):\n",
    "        relevant_docs = compression_retriever.invoke(question)\n",
    "        answer=''\n",
    "        for rd in relevant_docs:\n",
    "            answer += rd.page_content\n",
    "        rerank_answers.append(answer[:245])\n",
    "    return rerank_answers\n",
    "\n",
    "questions = list(query['question'].values)\n",
    "rerank_answers = rerank(questions, ensemble_retriever)\n",
    "print(rerank_answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提交"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:29.648479200Z",
     "start_time": "2024-11-07T17:51:49.193140Z"
    }
   },
   "outputs": [],
   "source": [
    "def emb(answers, emb_batch_size = 4):\n",
    "    model = SentenceTransformer(EMB_MODEL, trust_remote_code=True)\n",
    "    all_sentence_embeddings = []\n",
    "    for i in tqdm(range(0, len(answers), emb_batch_size), desc=\"embedding sentences\"):\n",
    "        batch_sentences = answers[i:i+emb_batch_size]\n",
    "        sentence_embeddings = model.encode(batch_sentences, normalize_embeddings=True)\n",
    "        all_sentence_embeddings.append(sentence_embeddings)\n",
    "    all_sentence_embeddings = np.concatenate(all_sentence_embeddings, axis=0)\n",
    "    print('emb_model max_seq_length: ', model.max_seq_length)\n",
    "    print('emb_model embeddings_shape: ', all_sentence_embeddings.shape[-1])\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return all_sentence_embeddings\n",
    "\n",
    "all_sentence_embeddings = emb(rerank_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:59:29.648479200Z",
     "start_time": "2024-11-07T17:51:52.309769Z"
    }
   },
   "outputs": [],
   "source": [
    "sub['answer'] = rerank_answers\n",
    "sub['embedding']= [','.join([str(a) for a in all_sentence_embeddings[i]]) for i in range(len(all_sentence_embeddings))]\n",
    "sub.to_csv('submit3.0.csv', index=None)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5782812,
     "sourceId": 9501900,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5819690,
     "sourceId": 9551539,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5831812,
     "sourceId": 9568175,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "train310",
   "language": "python",
   "name": "train39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
